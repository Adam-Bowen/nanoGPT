# nanoGPT

üåê nanoGPT (Andrej Karpathy's Zero to Hero)

## Overview

Implements a small-scale Generative Pretrained Transformer (GPT) model, following the paper "Attention is All You Need" and the architecture of OpenAI's GPT-2/ GPT-3 models.

## Implementation Details

The model is implemented using PyTorch and follows these main components:

1. Token and positional embeddings
2. Multi-layer transformer blocks
3. Layer normalization
4. Feed-forward neural networks
5. Softmax output layer for next token prediction

## References

- Vaswani, A., et al. (2017). "Attention is All You Need"
- OpenAI's GPT-2 and GPT-3 technical reports
- Andrej Karpathy's "Zero to Hero" series
